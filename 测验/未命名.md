


# 部署openstack
网卡最好用NAT模式
# 部署单节点ceph(两台)
在osp的物理机上为cs01和cs02两台机器配置两套P版16.2.11的单节点ceph集群，每套集群可以使用9个OSD，无需部署dashboard和监控相关组件，仅部署mon和mgr以及OSD和相关必要组件，通过172.17.0.1的mon可以访问并使用cs01的集群，通过172.17.0.2的mon可以访问到第二套集群，其中cs01的集群为生产集群，cs02为灾备集群
## 部署流程
- vmware虚拟机准备，两台，NAT，除系统盘外再加9块硬盘
- 安装必要组件
```shell
yum -y install python3
yum -y install lvm2
yum -y install podman
```
- 配置时间同步
```shell
cat >/etc/chrony.conf <<EOF
pool ntp.aliyun.com iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
allow 172.16.1.0/24
keyfile /etc/chrony.keys
leapsectz right/UTC
logdir /var/log/chrony
EOF

systemctl restart chronyd
systemctl enable chronyd
chronyc sources
```
- 下载cephadm脚本并安装
```shell
# 下载
wget http://lab4.cn/cephadm
# 添加存储库
./cephadm add-repo --release pacific
# 安装cephadm
./cephadm install
```
- 引导ceph集群
```shell
# 注意--mon-ip修改为本机IP
./cephadm bootstrap --mon-ip 172.18.0.20 --allow-fqdn-hostname --skip-monitoring-stack --skip-dashboard
```
>[!tip]
>`--allow-fqdn-hostname` 允许使用主机名进行节点识别
>`--skip-monitoring-stack` 跳过 Ceph 集群的监控栈部署
>`--skip-dashboard` 跳过 **Ceph Dashboard** 部署

- 安装ceph的客户端软件
```shell
yum -y install ceph-common
```
- 添加osd
```shell
ceph orch apply osd --all-available-devices
```
- 修改故障域为OSD
```shell
# 进入ceph集群管理容器
cephadm shell
ceph osd getcrushmap >> crushmap.bin
crushtool -d crushmap.bin >> crushmap.txt
vi crushmap.txt

# 将‘# rule’下第6行的`host`改为`osd`
rule replicated_rule {
        id 0
        type replicated
        min_size 1
        max_size 10
        step take default
        step chooseleaf firstn 0 type osd
        step emit
}

rm -f crushmap.bin
crushtool -c crushmap.txt -o crushmap.bin
ceph osd setcrushmap -i crushmap.bin
```
## 使用前准备
### 创建rbd存储池及用户
在cs01为ops和Mysql分别配置名为cinder-pool和mysql-pool的存储池，存储池使用RBD类型的存储，存储空间分别配置10GB大小，使用3副本保障数据的安全性，并创建用于对接ceph存储中RBD连接的普通用户，用户名设置为你姓名全拼的用户（例如：姓名张三，账号设置：zhangsan），对cinder-pool和 mysql-pool 存储池具有读写权限。
```shell
# 创建存储池
ceph osd pool create cinder-pool
ceph osd pool create mysql-pool
# 创建标签
ceph osd pool application enable cinder-pool rbd
ceph osd pool application enable mysql-pool rbd
# 添加用户
ceph auth add client.zhangmingming mon 'allow r' osd 'allow rwx'
# 创建mysql的rbd映像
rbd create mysql-pool/mysql-data --size 10G
```
### 创建cephFS存储池并添加用户权限
在cs01为Kubernetes配置一个名为kubernetes-pool的存储池，该存储池使用CephFS类型的存储配置一个k8s_fs的文件系统存储，使用3副本保障数据的安全性，并创建用于对接ceph存储中CephFS连接的普通用户，用户名设置为你姓名全拼的用户（例如：姓名张三，账号设置：zhangsan），对kubernetes-pool存储池的具有读写权限。
```shell
# 创建数据池
ceph osd pool create kubernetes-data
# 创建元数据池
ceph osd pool create kubernetes-metadata
# 数据池配置cephfs类型
ceph osd pool application enable kubernetes-data cephfs
ceph osd pool application enable kubernetes-metadata cephfs
# 构建一个名为k8s_fs的文件系统，使用kubernetes-metadata作为元数据池，使用kubernetes-data作为数据池
ceph fs new k8s_fs kubernetes-metadata kubernetes-data
#修改用户的权限，增加对mds的读写权限
ceph auth caps client.zhangmingming mon "allow r" osd "allow rwx" mds "allow rw" mgr "allow rw"
# 部署一个MDS
ceph orch apply mds k8s_fs --placement=1
```
### 云硬盘容灾
为OpenStack配置云硬盘多站点容灾，将CS01生产站点中的cinder_pool存储池中的RBD通过RBD镜像Mirror的方式同步到远程灾备站点CS02，同步的模式为单向模式，镜像复制方式为池模式
```shell
# CS01集群操作:
rbd mirror pool enable cinder-pool pool # 开启存储池同步，同步模式为池模式
rbd mirror pool peer bootstrap create --site-name cs01 cinder-pool > /opt/cs01.key # 创建集群秘钥并导出
scp /opt/cs01.key root@cs02:/opt/ # 发送key到cs02的灾备集群
rbd feature enable cinder-pool/volume-27147c92-e11b-4c52-8bba-af82088ada58 journaling # 开启目标RBD同步特性

# CS02集群操作:
ceph osd pool create cinder-pool # 创建存储池和主集群相同
ceph osd pool application enable cinder-pool rbd # 为存储池创建标签
ceph orch apply rbd-mirror --placement=1 # 安装rbd-mirror
rbd mirror pool peer bootstrap import --site-name cs02 --direction rx-only cinder-pool /opt/cs01.key # 导入cs01的集群秘钥
rbd -p cinder-pool ls # 查看存储池卷是否同步
```
# cinder对接ceph
## ceph节点操作
```shell
# 导出cinder密钥
ceph auth get client.zhangmingming -o /etc/ceph/ceph.client.zhangmingming.keyring
```
## openstack操作
### 所有节点
```shell
# 创建ceph配置文件目录
mkdir /etc/ceph/
# 拷贝密钥和配置文件
scp root@192.168.44.111:/etc/ceph/

```
# 本地yum源，harbor镜像仓库
# k8s集群搭建
# mysql部署
# mysql对接ceph
# discuz容器发布
# prometheus


