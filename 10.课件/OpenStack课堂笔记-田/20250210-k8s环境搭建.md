问题：
可能有成千上万个容器。docker compose
这些容器都是相互独立的。一旦挂了。业务退出了。

docker swarm
kubernetes

2014年的时候开源的。

# 1. kubernetes 部署方式
- k8s 二进制部署（课后作业，自行操作，纯纯体力活）
- kubeadm 部署工具
- kubesparay 基于ansible部署的，但是不好更改镜像，写死的。
- kubekey 国内青云开源的，用的不多
- sealos 阿里孵化的一个项目，后来独立出来的，工具用起来很简单。一条命令即可搞定。（通过 helm）
- 自己编写shell脚本（引用kubeadm）
- 图形化界面搭建：tkestack，腾讯云开源的（感兴趣可以看一下）

## 1.1 sealos 部署
- 准备3个节点（8.4 minimal）
- 下载sealos
sealos:非常快速和方便

https://sealos.run/docs/self-hosting/lifecycle-management/quick-start/deploy-kubernetes
https://github.com/labring/sealos/releases/tag/v5.0.1

- 解压
```shell
tar -zxvf sealos_5.0.1_linux_amd64.tar.gz
```
- 把可执行文件放到 /usr/bin下面
```shell
mv sealos sealctl lvscare image-cri-shim /usr/bin/
```
**单实例/单 master 部署**
``` shell
sealos run \
registry.cn-shanghai.aliyuncs.com/labring/helm:v3.16.2 \
registry.cn-shanghai.aliyuncs.com/labring/calico:v3.28.1 \
registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.30.0 \
--masters 192.168.44.201 \
--nodes 192.168.44.202,192.168.44.203 \
-u root \
-p redhat
```
> #备注 
> - 镜像从 [dockerhub](registry.cn-shanghai.aliyuncs.com) 中拉取

**多实例/多 master 部署**
```shell
sealos run \
registry.cn-shanghai.aliyuncs.com/labring/helm:v3.16.2 \
registry.cn-shanghai.aliyuncs.com/labring/calico:v3.28.1 \
registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.30.0 \
--masters 192.168.44.100,192.168.44.101,192.168.44.102 \
--nodes 192.168.44.201,192.168.44.202 \
-u root \
-p redhat
```

> #问题 为什么是 3个 master 而不是 2 个或 4 个？ 
> - 因为 etcd 选举存在过半存活机制，如果是 2 个 master，则不允许任何节点挂掉，如果是 4 个 master，则只允许挂 1 个，和 3 个 master 相同

## 1.2 kubeadm 部署
注意：k8s部署安装的时候，master节点cpu必须至少2个起
### 配置主机名和ip映射（所有节点）
```shell
cat << EOF >> /etc/hosts
192.168.44.201 master
192.168.44.202 node1
192.168.44.203 node2
EOF
```
### 关闭防火墙 （所有）
```shell
systemctl stop firewalld
systemctl disable firewalld
setenforce 0
sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
swapoff -a
sed -i "s/^.*swap/#&/g" /etc/fstab
iptables -F
```
### 配置yum源（所有）
```shell
sed -e "s|^mirrorlist=|#mirrorlist=|g" -e "s|^#baseurl=http://mirror.centos.org/\$contentdir/\$releasever|baseurl=https://mirrors.aliyun.com/centos-vault/8.4.2105|g" -i.bak /etc/yum.repos.d/CentOS-*.repo
```
### 安装基础软件包（所有）
```shell
yum install -y vim bash-completion net-tools wget chrony tar yum-utils
```
### 配置时钟源（master）
```shell
cp /etc/chrony.conf /etc/chrony.conf.bak
cat << EOF > /etc/chrony.conf
server master iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
allow 192.168.44.0/24
local stratum 10
keyfile /etc/chrony.keys
leapsectz right/UTC
logdir /var/log/chrony
EOF

systemctl enable chronyd.service
systemctl start chronyd.service
```
### 配置时钟源（node）
```shell
cp /etc/chrony.conf /etc/chrony.conf.bak
cat <<'EOF2' > /etc/chrony.conf
server master iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
keyfile /etc/chrony.keys
leapsectz right/UTC
logdir /var/log/chrony
EOF2

systemctl enable chronyd.service
systemctl start chronyd.service
```
### 开启网桥转发（所有）
```shell
sed -i 's/net.ipv4.ip_forward=0/net.ipv4.ip_forward=1/g' /etc/sysctl.conf
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
EOF
```
### 安装docker-ce（所有）
大家一定要明白，这里我安装docker-ce的目的，是为了大家后续多一个docker学习实验环境。因为安装好docker之后，本身自带containerd
本来k8s安装的时候只需要对接满足CRI接口的容器运行时（containerd）即可，只需要单独安装containerd即可。
```shell
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum list docker-ce --showduplicates | sort -r
yum install -y docker-ce
systemctl enable docker
systemctl start docker
```
### 配置端点服务（所有）
```shell
cat << EOF > /etc/crictl.yaml 
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 5
debug: false
EOF
```

### 修改cgroup类型（所有）
```shell
containerd config default > /etc/containerd/config.toml
sed -i "s#registry.k8s.io/pause#registry.aliyuncs.com/google_containers/pause#g" /etc/containerd/config.toml
sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
systemctl restart containerd
systemctl enable containerd
```
### 配置k8s 源（所有）
```shell
cat << EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/rpm/
enabled=1
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/rpm/repodata/repomd.xml.key
EOF
```
### 安装集群工具（所有）
```shell
yum install -y kubelet-1.30.0 kubeadm-1.30.0 kubectl-1.30.0 --disableexcludes=kubernetes
systemctl enable --now kubelet
```
> #备注
> CentOS8.4 不支持 1.32.0，使用 1.30.0
### 通过kubeadm初始化集群（master）
```shell
kubeadm init --image-repository registry.aliyuncs.com/google_containers --kubernetes-version=v1.30.0 --pod-network-cidr=10.244.0.0/16
```
- `pod-network-cidr`：容器的网络地址
> #备注
> CentOS8.4 不支持 1.32.0，使用 1.30.0
### 配置环境变量（master）
```shell
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
echo 'export KUBECONFIG=/etc/kubernetes/admin.conf' >> /etc/profile
```

```
[root@master ~]# kubectl get node
NAME     STATUS     ROLES           AGE     VERSION
master   NotReady   control-plane   4m10s   v1.30.0
```

### 将其他node加入当前集群（node）
将集群master节点生成的join语句复制到对应的node节点上执行。
```shell
[root@node1 ~]# kubeadm join 192.168.44.201:6443 --token izvenj.k5huu3jivdo1kwwl \
>         --discovery-token-ca-cert-hash sha256:70f1e9868d64d4dc086749aa1c9faecb1c7074736ae249ffdf7e1355b8ceabf3
```
```
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 1.001395756s
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```


```shell
[root@node2 ~]# kubeadm join 192.168.44.201:6443 --token izvenj.k5huu3jivdo1kwwl \
>         --discovery-token-ca-cert-hash sha256:70f1e9868d64d4dc086749aa1c9faecb1c7074736ae249ffdf7e1355b8ceabf3
```
```
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 1.002344729s
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

最后回到master上查看集群
```shell
[root@master ~]# kubectl get node
NAME     STATUS     ROLES           AGE     VERSION
master   NotReady   control-plane   4m57s   v1.30.0
node1    NotReady   <none>          9s      v1.30.0
node2    NotReady   <none>          5s      v1.30.0
```

现在节点加入到集群中了，但是集群状态为 NotReady 未就绪，原因是因为没有CNI网络插件，k8s官方没有实现网络插件功能，该功能都是由三方网络厂商来提供的。
所以现在要配置网络插件，实现容器跨界点通信。
目前生产环境使用的网络插件有2个：calico / Cilium

### 拉取calico镜像(所有)
提供了两种方式：
1.直接将离线镜像包通过ctr导入到k8s.io命名空间
https://blog.51cto.com/cloudcs/11372015

2.或者在线拉取，通过docker拉取镜像，导出镜像，通过ctr导入到k8s.io命名空间
master拉取后，将tar包拷贝到其他node上，进行导入即可
```shell
docker pull registry.cn-hangzhou.aliyuncs.com/gcontainer/operator:v1.34.0
docker pull registry.cn-hangzhou.aliyuncs.com/gcontainer/typha:v3.28.0
docker pull registry.cn-hangzhou.aliyuncs.com/gcontainer/kube-controllers:v3.28.0
docker pull registry.cn-hangzhou.aliyuncs.com/gcontainer/apiserver:v3.28.0
docker pull registry.cn-hangzhou.aliyuncs.com/gcontainer/cni:v3.28.0
docker pull registry.cn-hangzhou.aliyuncs.com/gcontainer/node-driver-registrar:v3.28.0
docker pull registry.cn-hangzhou.aliyuncs.com/gcontainer/csi:v3.28.0
docker pull registry.cn-hangzhou.aliyuncs.com/gcontainer/pod2daemon-flexvol:v3.28.0
docker pull registry.cn-hangzhou.aliyuncs.com/gcontainer/node:v3.28.0

docker tag registry.cn-hangzhou.aliyuncs.com/gcontainer/operator:v1.34.0 quay.io/tigera/operator:v1.34.0
docker tag registry.cn-hangzhou.aliyuncs.com/gcontainer/typha:v3.28.0 calico/typha:v3.28.0
docker tag registry.cn-hangzhou.aliyuncs.com/gcontainer/kube-controllers:v3.28.0 calico/kube-controllers:v3.28.0
docker tag registry.cn-hangzhou.aliyuncs.com/gcontainer/apiserver:v3.28.0 calico/apiserver:v3.28.0
docker tag registry.cn-hangzhou.aliyuncs.com/gcontainer/cni:v3.28.0 calico/cni:v3.28.0
docker tag registry.cn-hangzhou.aliyuncs.com/gcontainer/node-driver-registrar:v3.28.0 calico/node-driver-registrar:v3.28.0
docker tag registry.cn-hangzhou.aliyuncs.com/gcontainer/csi:v3.28.0 calico/csi:v3.28.0
docker tag registry.cn-hangzhou.aliyuncs.com/gcontainer/pod2daemon-flexvol:v3.28.0 calico/pod2daemon-flexvol:v3.28.0
docker tag registry.cn-hangzhou.aliyuncs.com/gcontainer/node:v3.28.0 calico/node:v3.28.0

docker rmi registry.cn-hangzhou.aliyuncs.com/gcontainer/operator:v1.34.0
docker rmi registry.cn-hangzhou.aliyuncs.com/gcontainer/typha:v3.28.0
docker rmi registry.cn-hangzhou.aliyuncs.com/gcontainer/kube-controllers:v3.28.0
docker rmi registry.cn-hangzhou.aliyuncs.com/gcontainer/apiserver:v3.28.0
docker rmi registry.cn-hangzhou.aliyuncs.com/gcontainer/cni:v3.28.0
docker rmi registry.cn-hangzhou.aliyuncs.com/gcontainer/node-driver-registrar:v3.28.0
docker rmi registry.cn-hangzhou.aliyuncs.com/gcontainer/csi:v3.28.0
docker rmi registry.cn-hangzhou.aliyuncs.com/gcontainer/pod2daemon-flexvol:v3.28.0
docker rmi registry.cn-hangzhou.aliyuncs.com/gcontainer/node:v3.28.0

mkdir /images
docker save quay.io/tigera/operator:v1.34.0 -o /images/operator-v1.34.0.tar
docker save calico/typha:v3.28.0 -o /images/typha-v3.28.0.tar
docker save calico/kube-controllers:v3.28.0 -o /images/kube-controllers-v3.28.0.tar
docker save calico/apiserver:v3.28.0 -o /images/apiserver-v3.28.0.tar
docker save calico/cni:v3.28.0 -o /images/cni-v3.28.0.tar
docker save calico/node-driver-registrar:v3.28.0 -o /images/node-driver-registrar-v3.28.0.tar
docker save calico/csi:v3.28.0 -o /images/csi-v3.28.0.tar
docker save calico/pod2daemon-flexvol:v3.28.0 -o /images/pod2daemon-flexvol-v3.28.0.tar
docker save calico/node:v3.28.0 -o /images/node-v3.28.0.tar

ctr -n k8s.io image import /images/apiserver-v3.28.0.tar
ctr -n k8s.io image import /images/cni-v3.28.0.tar
ctr -n k8s.io image import /images/csi-v3.28.0.tar
ctr -n k8s.io image import /images/kube-controllers-v3.28.0.tar
ctr -n k8s.io image import /images/node-driver-registrar-v3.28.0.tar
ctr -n k8s.io image import /images/node-v3.28.0.tar
ctr -n k8s.io image import /images/operator-v1.34.0.tar
ctr -n k8s.io image import /images/pod2daemon-flexvol-v3.28.0.tar
ctr -n k8s.io image import /images/typha-v3.28.0.tar
```

```shell
[root@node1 ~]#mkdir /images
[root@node2 ~]#mkdir /images
[root@master ~]# scp -r /images/*.tar node1:/images/
[root@master ~]# scp -r /images/*.tar node2:/images/
```


之后在node1上和node2上分别进行镜像导入
```shell
ctr -n k8s.io image import /images/apiserver-v3.28.0.tar
ctr -n k8s.io image import /images/cni-v3.28.0.tar
ctr -n k8s.io image import /images/csi-v3.28.0.tar
ctr -n k8s.io image import /images/kube-controllers-v3.28.0.tar
ctr -n k8s.io image import /images/node-driver-registrar-v3.28.0.tar
ctr -n k8s.io image import /images/node-v3.28.0.tar
ctr -n k8s.io image import /images/operator-v1.34.0.tar
ctr -n k8s.io image import /images/pod2daemon-flexvol-v3.28.0.tar
ctr -n k8s.io image import /images/typha-v3.28.0.tar
```

执行官方提供的2个脚本，operator和custom

### 执行calico脚本（master）
```shell
kubectl create -f tigera-operator-v3.28.0.yaml
kubectl create -f custom-resources-v3.28.0.yaml
```
![[100.附件/tigera-operator-v3.28.0.yaml]]
![[100.附件/custom-resources-v3.28.0.yaml]]

最终看到所有的容器都是running，整个集群则正常使用
```shell
kubectl get pod -n calico-system
```

大家先按照上述流程操作，大概率是我自己环境问题，课后我还原重新走一遍。
