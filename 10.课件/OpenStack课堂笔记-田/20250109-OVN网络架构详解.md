内容回顾
1.keystone（概念）
2.neutron 概念
tap/tun/veth pair
linux bridge(brctl show)/OVS(ovs-vsctl show)
namespace:逻辑隔离
linux 6个ns：
● Mount Namespace：用于隔离文件系统挂载点
● UTS Namespace：用来隔离主机名和域名，UNIX Timesharing System
● IPC Namespace： 用于进程间通信的
● PID Namespace： 用来隔离进程ID
● Network Namespace：用于隔离Linux系统的设备，以及IP地址、端口、路由表、防火墙规则等网络资源
● User Namespace：用于隔离用户权限

当前openstack环境使用的网络类型 geneve 隧道（vxlan）

1.ovs
一个完整os，包含两个部分：内核空间+用户空间
内核空间：内存管理、驱动管理、文件系统等
用户空间：库和插件、应用等
流表：就是一系列数据转发规则

之前openstack版本，用的OVS来进行数据规则配置和转发，现在新版本openstack，通过应答文件配置OVN后，使用的是OVN架构（OVN依赖ovs）

1）用一台linux安装ovs
systemctl stop firewalld.service 
systemctl disable firewalld.service
setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config

mkdir /etc/yum.repos.d/bak
mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/
cat <<EOF > /etc/yum.repos.d/cloudcs.repo
[BaseOS]
name = BaseOS
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/BaseOS/x86_64/os/
gpgcheck = 0

[AppStream]
name = AppStream
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/AppStream/x86_64/os/
gpgcheck = 0

[centos-advanced-virtualization]
name = centos-advanced-virtualization
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/virt/x86_64/advanced-virtualization/
gpgcheck = 0

[centos-ceph-pacific]
name = centos-ceph-nautilus
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/storage/x86_64/ceph-pacific/
gpgcheck = 0

[centos-nfv-openvswitch]
name = centos-nfv-openvswitch
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/nfv/x86_64/openvswitch-2/
gpgcheck = 0

[centos-openstack-victoria]
name = centos-openstack-victoria
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/cloud/x86_64/openstack-victoria/
gpgcheck = 0

[centos-rabbitmq-38]
name = centos-rabbitmq-38
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/messaging/x86_64/rabbitmq-38/
gpgcheck = 0

[extras]
name = extras
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/extras/x86_64/os/
gpgcheck = 0

[PowerTools]
name = PowerTools
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/PowerTools/x86_64/os/
gpgcheck = 0
EOF

yum clean all
yum repolist all
yum install -y vim net-tools bash-completion openvswitch2.1*

2）启动服务并更改指定路径
[root@ovs ~]# systemctl start openvswitch
[root@ovs ~]# echo 'export PATH=$PATH:/usr/share/openvswitch/scripts' >> /etc/profile
[root@ovs ~]# source /etc/profile

3）创建一个ovs交换机
[root@ovs ~]# ovs-vsctl add-br sw1
[root@ovs ~]# ovs-vsctl show


4）创建两个网络ns
[root@ovs ~]# ip netns add ns1
[root@ovs ~]# ip netns add ns2
[root@ovs ~]# ip netns
ns2
ns1

5）创建两对儿 veth
创建一对儿veth veth11和veth12，把veth12帮给ns1并启动
[root@ovs ~]# ip link add veth11 type veth peer name veth12
[root@ovs ~]# ip link set veth12 netns ns1
[root@ovs ~]# ip netns exec ns1 ip link set veth12 up

创建一对儿veth21和veth22，把veth22帮给ns2并启动
[root@ovs ~]# ip link add veth21 type veth peer name veth22
[root@ovs ~]# ip link set veth22 netns ns2
[root@ovs ~]# ip netns exec ns2 ip link set veth22 up

6）把11和21添加到ovs上
[root@ovs ~]# ip link set veth11 up
[root@ovs ~]# ip link set veth21 up
[root@ovs ~]# ovs-vsctl add-port sw1 veth11
[root@ovs ~]# ovs-vsctl add-port sw1 veth21

7）为ns配置ip（相当于给虚拟网卡配置ip）
[root@ovs ~]# ip netns exec ns1 ip addr add 1.1.1.1/24 dev veth12
[root@ovs ~]# ip netns exec ns1 ifconfig veth12
veth12: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 1.1.1.1  netmask 255.255.255.0  broadcast 0.0.0.0
        inet6 fe80::e0de:21ff:feb2:1b1e  prefixlen 64  scopeid 0x20<link>
        ether e2:de:21:b2:1b:1e  txqueuelen 1000  (Ethernet)
        RX packets 13  bytes 1006 (1006.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 11  bytes 866 (866.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

[root@ovs ~]# ip netns exec ns2 ip addr add 1.1.1.2/24 dev veth22
[root@ovs ~]# ip netns exec ns2 ifconfig veth22
veth22: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 1.1.1.2  netmask 255.255.255.0  broadcast 0.0.0.0
        inet6 fe80::e865:8ff:fe88:8901  prefixlen 64  scopeid 0x20<link>
        ether ea:65:08:88:89:01  txqueuelen 1000  (Ethernet)
        RX packets 13  bytes 1006 (1006.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 11  bytes 866 (866.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

[root@ovs ~]# ip netns exec ns1 ping -c 3 1.1.1.2
PING 1.1.1.2 (1.1.1.2) 56(84) bytes of data.
64 bytes from 1.1.1.2: icmp_seq=1 ttl=64 time=0.267 ms
64 bytes from 1.1.1.2: icmp_seq=2 ttl=64 time=0.048 ms
64 bytes from 1.1.1.2: icmp_seq=3 ttl=64 time=0.038 ms

--- 1.1.1.2 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2054ms
rtt min/avg/max/mdev = 0.038/0.117/0.267/0.106 ms

8）模拟vlan
[root@ovs ~]# ovs-vsctl set port veth11 tag=10
[root@ovs ~]# ovs-vsctl set port veth21 tag=20
[root@ovs ~]#
[root@ovs ~]# ip netns exec ns1 ping -c 3 1.1.1.2
PING 1.1.1.2 (1.1.1.2) 56(84) bytes of data.


[root@ovs ~]# ovs-vsctl set port veth21 tag=10
[root@ovs ~]# ovs-vsctl show
c2ff17ab-34f3-4cd2-9a6c-77e800325e00
    Bridge br-int
        Port br-int
            Interface br-int
                type: internal
    Bridge sw1
        Port veth21
            tag: 10
            Interface veth21
        Port sw1
            Interface sw1
                type: internal
        Port veth11
            tag: 10
            Interface veth11
    ovs_version: "2.13.6"
[root@ovs ~]# ip netns exec ns1 ping -c 3 1.1.1.2
PING 1.1.1.2 (1.1.1.2) 56(84) bytes of data.
64 bytes from 1.1.1.2: icmp_seq=1 ttl=64 time=0.301 ms
64 bytes from 1.1.1.2: icmp_seq=2 ttl=64 time=0.041 ms
64 bytes from 1.1.1.2: icmp_seq=3 ttl=64 time=0.038 ms

--- 1.1.1.2 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2049ms
rtt min/avg/max/mdev = 0.038/0.126/0.301/0.123 ms

9）模拟流表规则
[root@ovs ~]# ovs-ofctl dump-flows sw1
 cookie=0x0, duration=1231.834s, table=0, n_packets=34, n_bytes=2492, priority=0 actions=NORMAL
[root@ovs ~]#

[root@ovs ~]# ovs-ofctl add-flow sw1 "priority=2,in_port=veth11,actions=drop"
[root@ovs ~]# ovs-ofctl dump-flows sw1
 cookie=0x0, duration=1.760s, table=0, n_packets=0, n_bytes=0, priority=2,in_port=veth11 actions=drop
 cookie=0x0, duration=1469.562s, table=0, n_packets=34, n_bytes=2492, priority=0 actions=NORMAL
[root@ovs ~]#
[root@ovs ~]# ip netns exec ns1 ping -c 3 1.1.1.2
PING 1.1.1.2 (1.1.1.2) 56(84) bytes of data.

为何ping不通，因为流量从veth12出去，达到交换机sw1的时候，流量需要通过veth11进入交换机内部，而在交换机里面我配置了一条规则（流表项），凡是从veth11进入的流量，直接drop掉。

[root@ovs ~]# ovs-ofctl del-flows sw1 "in_port=veth11"
[root@ovs ~]# ip netns exec ns1 ping -c 3 1.1.1.2
PING 1.1.1.2 (1.1.1.2) 56(84) bytes of data.
64 bytes from 1.1.1.2: icmp_seq=1 ttl=64 time=0.416 ms
64 bytes from 1.1.1.2: icmp_seq=2 ttl=64 time=0.040 ms
64 bytes from 1.1.1.2: icmp_seq=3 ttl=64 time=0.042 ms

--- 1.1.1.2 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2041ms
rtt min/avg/max/mdev = 0.040/0.166/0.416/0.176 ms

关于OVS的后续拓展实验，大家可参考这篇文章
https://mp.weixin.qq.com/s/KVtja9QUTbnyx77tSzTW3g

2.OVN
在课前提问了一个问题，就是位于不同计算节点上的不同虚拟机，他俩是如何互通的？先来了解下OVN架构。
OVN架构包括控制面和数据面。
2.1了解OVN架构
分为两大类：
中心节点（管理节点）：CMS-->CMS plugin-->northbound DB-->northd-->sounthbound DB
hypervisor节点（计算节点）：ovn-controller(对接 ovs-vswitchd/ovsdb-server)

2.2搭建OVN
ovn是建立在ovs基础上的，所以先要安装ovs，再安装ovn

1）克隆两台linux，安装基础包及ovs
systemctl stop firewalld.service 
systemctl disable firewalld.service
setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config

mkdir /etc/yum.repos.d/bak
mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/
cat <<EOF > /etc/yum.repos.d/cloudcs.repo
[BaseOS]
name = BaseOS
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/BaseOS/x86_64/os/
gpgcheck = 0

[AppStream]
name = AppStream
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/AppStream/x86_64/os/
gpgcheck = 0

[centos-advanced-virtualization]
name = centos-advanced-virtualization
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/virt/x86_64/advanced-virtualization/
gpgcheck = 0

[centos-ceph-pacific]
name = centos-ceph-nautilus
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/storage/x86_64/ceph-pacific/
gpgcheck = 0

[centos-nfv-openvswitch]
name = centos-nfv-openvswitch
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/nfv/x86_64/openvswitch-2/
gpgcheck = 0

[centos-openstack-victoria]
name = centos-openstack-victoria
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/cloud/x86_64/openstack-victoria/
gpgcheck = 0

[centos-rabbitmq-38]
name = centos-rabbitmq-38
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/messaging/x86_64/rabbitmq-38/
gpgcheck = 0

[extras]
name = extras
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/extras/x86_64/os/
gpgcheck = 0

[PowerTools]
name = PowerTools
baseurl = https://mirrors.aliyun.com/centos-vault/8.4.2105/PowerTools/x86_64/os/
gpgcheck = 0
EOF

yum clean all
yum repolist all
yum install -y vim net-tools bash-completion openvswitch2.1*

安装好ovs后，不需要手工启服务

2）安装ovn

[root@ovn1 ~]# yum install -y ovn2.13*
[root@ovn2 ~]# yum install -y ovn2.13*

加载环境变量
echo 'export PATH=$PATH:/usr/share/ovn/scripts:/usr/share/openvswitch/scripts' >> /etc/profile
source /etc/profile

3）启动中心节点服务（只需要第一个节点 控制节点）
北向数据库--northd--南向数据库
[root@ovn1 ~]# ovn-ctl start_northd
/etc/ovn/ovnnb_db.db does not exist ... (warning).
Creating empty database /etc/ovn/ovnnb_db.db               [  OK  ]
Starting ovsdb-nb                                          [  OK  ]
/etc/ovn/ovnsb_db.db does not exist ... (warning).
Creating empty database /etc/ovn/ovnsb_db.db               [  OK  ]
Starting ovsdb-sb                                          [  OK  ]
Starting ovn-northd                                        [  OK  ]

4）启动计算节点服务（两个节点）
ovs-ctl start --system-id=random

[root@ovn1 ~]# ovs-ctl start --system-id=random
/etc/openvswitch/conf.db does not exist ... (warning).
Creating empty database /etc/openvswitch/conf.db           [  OK  ]
Starting ovsdb-server                                      [  OK  ]
Configuring Open vSwitch system IDs                        [  OK  ]
Inserting openvswitch module                               [  OK  ]
Starting ovs-vswitchd                                      [  OK  ]
Enabling remote OVSDB managers                             [  OK  ]

[root@ovn2 ~]# ovs-ctl start --system-id=random
/etc/openvswitch/conf.db does not exist ... (warning).
Creating empty database /etc/openvswitch/conf.db           [  OK  ]
Starting ovsdb-server                                      [  OK  ]
Configuring Open vSwitch system IDs                        [  OK  ]
Inserting openvswitch module                               [  OK  ]
Starting ovs-vswitchd                                      [  OK  ]
Enabling remote OVSDB managers                             [  OK  ]

5）启动ovn-controller（两个节点）

[root@ovn1 ~]# ovn-ctl start_controller
Starting ovn-controller                                    [  OK  ]

[root@ovn2 ~]# ovn-ctl start_controller
Starting ovn-controller                                    [  OK  ]

6）两个ovn-controller对接南向数据库

首先在控制节点上开放端口
[root@ovn1 ~]# ovn-nbctl set-connection ptcp:6641:192.168.44.241
[root@ovn1 ~]# ovn-sbctl set-connection ptcp:6642:192.168.44.241

在ovn1上执行命令
[root@ovn1 ~]# ovs-vsctl set Open_vSwitch . external-ids:ovn-remote="tcp:192.168.44.241:6642" external-ids:ovn-encap-ip="192.168.44.241" external-ids:ovn-encap-type=geneve external-ids:system-id=ovn1

在ovn2上执行命令
[root@ovn2 ~]# ovs-vsctl set Open_vSwitch . external-ids:ovn-remote="tcp:192.168.44.241:6642" external-ids:ovn-encap-ip="192.168.44.242" external-ids:ovn-encap-type=geneve external-ids:system-id=ovn2

在ovn1上查询
[root@ovn1 ~]# ovn-sbctl show
Chassis ovn2
    hostname: ovn2
    Encap geneve
        ip: "192.168.44.242"
        options: {csum="true"}
Chassis ovn1
    hostname: ovn1
    Encap geneve
        ip: "192.168.44.241"
        options: {csum="true"}

2.3 简单实践
1）ovn1操作
ip netns
ip netns add ns1
ip link add veth11 type veth peer name veth12
ip link set veth12 netns ns1
ip link set veth11 up
ip netns exec ns1 ip link set veth12 address 00:00:00:00:00:01
ip netns exec ns1 ip link set veth12 up
ovs-vsctl add-port br-int veth11
ip netns exec ns1 ip addr add 192.168.1.10/24 dev veth12

2）ovn2操作
ip netns
ip netns add ns1
ip link add veth11 type veth peer name veth12
ip link set veth12 netns ns1
ip link set veth11 up
ip netns exec ns1 ip link set veth12 address 00:00:00:00:00:02
ip netns exec ns1 ip link set veth12 up
ovs-vsctl add-port br-int veth11
ip netns exec ns1 ip addr add 192.168.1.20/24 dev veth12

当你在openstack中，创建了一个网络（相当于创建了一个分布式虚拟交换机，分布式虚拟交换机横跨整个集群），对外表现形式为网络，这个信息就记录在哪里？

默认ns1 和 ns2 是不通的。

3）打通
左边ovn1上
ovn-nbctl ls-add ls1
ovn-nbctl lsp-add ls1 ls1-node1-ns1
ovn-nbctl lsp-set-addresses ls1-node1-ns1 00:00:00:00:00:01
ovn-nbctl lsp-set-port-security ls1-node1-ns1 00:00:00:00:00:01

ovn-nbctl lsp-add ls1 ls1-node2-ns1
ovn-nbctl lsp-set-addresses ls1-node2-ns1 00:00:00:00:00:02
ovn-nbctl lsp-set-port-security ls1-node2-ns1 00:00:00:00:00:02

ovs-vsctl set interface veth11 external-ids:iface-id=ls1-node1-ns1

右边ovn2
ovs-vsctl set interface veth11 external-ids:iface-id=ls1-node2-ns1

测试
[root@ovn1 ~]# ip netns exec ns1 ping -c 5 192.168.1.20
PING 192.168.1.20 (192.168.1.20) 56(84) bytes of data.
64 bytes from 192.168.1.20: icmp_seq=1 ttl=64 time=0.296 ms
64 bytes from 192.168.1.20: icmp_seq=2 ttl=64 time=0.296 ms
64 bytes from 192.168.1.20: icmp_seq=3 ttl=64 time=0.589 ms
64 bytes from 192.168.1.20: icmp_seq=4 ttl=64 time=0.359 ms
64 bytes from 192.168.1.20: icmp_seq=5 ttl=64 time=0.493 ms

--- 192.168.1.20 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 4112ms
rtt min/avg/max/mdev = 0.296/0.406/0.589/0.118 ms

后续geneve隧道抓包验证、neutron架构，周六。