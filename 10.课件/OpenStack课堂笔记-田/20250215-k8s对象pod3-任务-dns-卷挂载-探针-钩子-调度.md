k8s pod
# pod 任务参数

在docker里面
dockerfile `CMD`/`Entrypoint`
`CMD`里面指定的内容可以作为“参数”传递给`entrypoint`
`CMD`指定任务可以被覆盖，但是`Entrypoint`是不可以被覆盖。

在k8s里面使用下面两个参数指定任务

`command`
`args`

k8s里面创建的pod，里面包含容器，容器是通过镜像创建的。

```shell
[root@master ~]# kubectl delete pod pod1
[root@master ~]# kubectl delete pods/pod1
[root@master ~]# kubectl delete pod pod{1,2,3}
[root@master ~]# kubectl delete pod --all
[root@master ~]#
根据文件里的资源定义删除pod
[root@master ~]# kubectl delete -f pod1.yaml
```


docker  cmd  Entrypoint
k8s     args command

在k8s里面如果你指定了command，这个命令可以覆盖镜像的entrypoint。

1.如果没有提供command和args参数，则默认使用镜像定义的值。
2.如果单独提供command，没有args参数，则仅使用k8s中定义的command，原有的镜像里面的cmd和entrypoint将被忽略。
3.如果单独提供args，没有command参数，则原有镜像里面的cmd被覆盖，而entrypoint将和args同时生效。
4.如果同时提供command和args，则忽略镜像中默认的cmd和entrypoint，只有command和args生效。

示例：
```shell
[root@node1 ~]# vim dockerfile
[root@node1 ~]# cat dockerfile
FROM alpine
COPY abc.sh /
CMD ["/bin/sh"]
ENTRYPOINT ["/abc.sh"]

[root@node1 ~]# vim abc.sh
[root@node1 ~]# cat abc.sh
#!/bin/sh
echo "this is Entrypoint" > /abc.txt
exec "$@"

chmod +x abc.sh

docker build -t alpine:v215 .

docker login
docker tag
docker push

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod1
  name: pod1
spec:
  containers:
  - image: swr.cn-north-4.myhuaweicloud.com/memeda/alpine:v215
    imagePullPolicy: IfNotPresent
    name: pod1
    resources: {}
    command: ["sh","-c","sleep 3600"]
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```
最终效果因为有command的存在，而忽略了镜像原有的cmd和entrypoint

# pod dnsPolicy
`None`：无DNS配置。使用该策略后，Pod 会使用其 dnsConfig 字段所提供的 DNS 设置
`Default`：使用默认配置。Pod中，默认是继承宿主机的DNS配置
`ClusterFirstWithHostNet`：对于以hostNetwork方式运行的Pod，应设置为此策略
`ClusterFirst`：默认配置。使用集群的DNS服务器，即coredns

- 如果pod中没有明确定义dnspolicy，默认使用的策略就是当前k8s集群内部的coredns地址（ClusterFirst）。
- 如果pod中指定了Default，继承宿主机的DNS
- 如果pod中指定了 hostNetwork,而此时还想使用集群内部的dns，那么dnspolicy需要指定为ClusterFirstWithHostNet
- 如果pod中需要使用自定义的dns地址，那么配置成None，并且通过dnsConfig字段进行配置


```shell
[root@master ~]# cat pod3.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod3
  name: pod3
spec:
  hostNetwork: true
  containers:
  - image: nginx
    name: pod3
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
[root@master ~]# kubectl exec -ti pod3 -- bash
root@node1:/# cat /etc/resolv.conf
nameserver 192.168.44.2
root@node1:/#
[root@master ~]# kubectl get pod -o wide
NAME   READY   STATUS    RESTARTS   AGE    IP               NODE    NOMINATED NODE   READINESS GATES
pod1   1/1     Running   0          13m    10.244.166.187   node1   <none>           <none>
pod2   1/1     Running   0          5m8s   10.244.166.188   node1   <none>           <none>
pod3   1/1     Running   0          6s     192.168.44.202   node1   <none>           <none>
```


会发现pod3，虽然定义了dnsPolicy: ClusterFirst，但是因着有hostNetwork: true，导致pod3没有使用集群内部的dns，现在想要实现一个效果：
1.pod 的ip地址集成节点的ip
2.pod 的dns使用集群内部的10.96

自定义DNS
https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config

```shell
apiVersion: v1
kind: Pod
metadata:
  namespace: default
  name: dns-example
spec:
  containers:
    - name: test
      image: nginx
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
      - 192.0.2.1 # this is an example
    searches:
      - ns1.svc.cluster-domain.example
      - my.dns.search.suffix
    options:
      - name: ndots
        value: "2"
      - name: edns0
```

# pod 卷挂载
docker里面 
```shell
docker run -tid --name os1 -v /abc centos:latest
docker run -tid --name os1 -v /aaa:/bbb centos:latest
```
`/abc` 为容器目录
`/aaa` 为宿主机目录，`/bbb` 为容器目录

`emptyDir`:容器所在的主机上会随机生成目录，将目录挂载到容器里面，删除pod，则随机目录也被删除；
`hostPath`:容器所在的主机上定义目录，并且将目录挂载到容器里面，删除pod，目录不会被删除；

定义方式：
1.单独定义一段volume
2.将这个volume挂载到容器里面

```shell
kubectl run pod1 --image nginx --image-pull-policy IfNotPresent --dry-run-client -o yaml > pod1.yaml
vim pod1.yaml

kubectl apply -f pod1.yaml
kubectl exec -ti pod1 -- bash
touch 666.txt
exit

kubectl get pod -o wide 
find / -name 666.txt

```
emptyDir示例
```shell
[root@master ~]# vim pod1.yaml
[root@master ~]# cat pod1.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod1
  name: pod1
spec:
  volumes:
  - name: v1
    emptyDir: {}
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: pod1
    resources: {}
    volumeMounts:
    - name: v1
      mountPath: /usr/share/nginx/html
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

hostPath示例
```shell
[root@master ~]# vim pod1.yaml
[root@master ~]# cat pod1.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod1
  name: pod1
spec:
  volumes:
  - name: v1
    emptyDir: {}
  - name: v2
    hostPath:
      path: /data
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: pod1
    resources: {}
    volumeMounts:
    - name: v2
      mountPath: /usr/share/nginx/html
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

# pod 初始化容器

初始化容器用于执行一些初始化操作，在主业务容器启动前，做一些初始化工作，比如改一些参数、生成一些配置文件等。但用的不多。
比如B服务要依赖于A服务，必须A服务先启动后，B服务才能正常运行。

一个pod里面可以有多个初始化容器，但是，一旦存在初始化容器，那么必须保证所有的初始化容器全部执行成功，最后才会运行主业务容器。
一旦初始化容器在启动过程中，出现了问题，后续主业务容器都不再执行。

需求：主业务容器使用nginx，启动前，需要生成一个index.html文件。

```shell
[root@master ~]# kubectl run pod1 --image nginx --image-pull-policy IfNotPresent --dry-run=client -o yaml > pod1.yaml
[root@master ~]# vim pod1.yaml
[root@master ~]# cat pod1.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod1
  name: pod1
spec:
  volumes:
  - name: v1
    emptyDir: {}
  initContainers:
  - name: busybox
    image: busybox
    volumeMounts:
    - name: v1
      mountPath: /tmp
    command:
    - sh
    - -c
    - "echo Hello initContainers > /tmp/index.html"
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: pod1
    resources: {}
    volumeMounts:
    - name: v1
      mountPath: /usr/share/nginx/html/
    ports:
    - name: http
      containerPort: 80
      hostPort: 5000
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

# pod 资源限制
注意理解`request`和`limit`
## limit
其中对于limit理解相对简单，limit限制的意思，一旦存在limits，那么会按照limit限制的值进行限定。
当然，有可能会存在这种情况
如果容器中存在进程消耗资源大于 limit 则会被终止

一个pod
容器1
```shell
    resources:
      limits:
        memory: "64Mi"
        cpu: "500m"
```
cpu：例如 500m 就是 500/1000=0.5 个 cpu，2000m 就是 2 个 cpu
容器2
```shell
    resources:
      limits:
        memory: "64Mi"
        cpu: "1000m"
```
容器3
```shell
    resources:
      limits:
        memory: "64Mi"
        cpu: "1000m"
```

一旦cpu资源出现不足，那么在k8s里面会根据优先级策略进行划分。
查看是否有 resources 限制，以及优先级
```shell
kubectl edit pod pod1
```
按照一个服务等级QOS(Quality of service) 类进行保障资源分配的优先级。优先级从前到后依次为：
`Guaranteed`：肯定的保障的。设置了resources（只设置了limits或requests和limits值相等的情况下）
`Burstable`：可变的，突发，爆发。设置了resources（只设置了requests，或requests和limits值不相等的情况下）
`BestEffort`：尽力而为，没有设置任何resources

## request
request 请求资源的意思，但是不要误解，不是个pod使用的，而是给调度器预留的资源。

kube-scheduler 在调度的过程中经过3个大阶段
1.预选（过滤Filtering，筛选可以创建的 pod）
2.优选（打分Scoring）
3.终选（绑定Binding）

假如节点的cpu一共只有1个
比如某个节点有3个pod，每个pod设置request 0.2（request总数0.6），但实际3个pod非常空闲，实际每个pod只用了0.1（实际使用0.3，这是实际还有0.7没用），
这时候，有个请求，要创建一个新的pod，cpu 0.5，kube-scheduler 会不会将这个新pod调度到该节点上。不会！
没有。

再比如，3个pod，每个pod request 0.2，其中两个pod很空闲，实际每个用0.1（实际剩余资源0.8），另外1个很繁忙，它需要申请到0.8，问？有没有概率申请到。
有。

# pod 探针


livenessProbe 存活探针
一旦检测到，比如丢失了文件、端口等，那么存活探针就会直接重启该pod来尝试解决问题。

示例：
```shell
[root@master ~]# vim pod1.yaml
[root@master ~]# cat pod1.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod1
  name: pod1
spec:
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: pod1
    resources: {}
    command:
    - /bin/bash
    - -c
    - touch /tmp/aaa; sleep 30; rm -f /tmp/aaa; sleep 3600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/aaa
      initialDelaySeconds: 2
      periodSeconds: 2
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```
`initialDelaySeconds: 2`：容器启动后 2 秒启用探针
`periodSeconds: 2`：每隔 2 秒检测一次


readinessProbe 就绪探针
探测到问题后，不会重启，会显示未就绪，通过svc，不要将流量路由到该pod上。

`livenessprobe`:存活探针
10s 探测一次，周期性探测

`readinessprobe`:就绪探针
3s 探测一次，周期性探测

问？假如有一个pod，pod启动需要大概60s，启动事件长，进行初始化配置，连接数据库，启动服务。。。
假如：这个pod里面有个应用，应用在启动的时候，会生成一些重要的配置文件，这些配置文件有可能是后续的pod使用的。可是启动过程很漫长。

`startupProbe`:启动探针
是为了保证pod的应用能够完整的启动，一旦检测到完全启动，启动探针退出，启动探针一旦探测成功，就退出了，不会再次运行了，之后的探测工作就交给了存活探针和就绪探针。
如果三个探针同时存在：startupprobe启动探针优先级最高，启动探针在执行过程中，其他探针不可以工作，必须等待启动探针退出。

探针支持的方法/类型：
默认情况下，kubernetes不对容器探活，当容器运行时返回容器就绪，kubernetes就认为容器就绪
探针类型：
1.基于tcp端口
2.基于http
3.自定义脚本 exec
4.基于gRPC端口

探活的时机：
startupProbe：启动阶段探活，探活失败是预期的状态；如果成功，则startupProbe退出；如果超过失败的最大次数，容器会被杀掉
livenessProbe：在运行周期内探活，预期是成功；如果失败，则杀死容器，在startupProbe结束后，启动livenessProbe探活
readinessProbe：在运行周期内探活，预期是成功，如果失败，则会将容器置为未就绪

探活的参数
initialDelaySecond：容器启动后等待多长时间后探活开始工作，默认是0，最小值是0
periodSeconds：执行探活任务的间隔，默认是10，最小值是1
timeoutSeconds：探活超时后多长时间后，探活失败，默认值是1，最小值是1，探活任务发出请求后，如果着这个时间内没有收到响应，则认为该任务失败
successThreshold：探活失败后，被视为成功的最小连续成功次数，默认是1，最小值是1
failureThreshold：探活失败时，k8s重试的次数，默认是3，最小值是1



# 生命周期钩子
生命周期钩子 poststart/prestop
主容器启动后，要不要做一些初始化操作；
主容器停止前，要不要做一些清尾工作；
它存在的目的，就是能够让pod关闭的更加优雅。
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250224015132850.png)
启动前（初始化容器）--启动中（startupProbe）--启动后（poststart）--运行中（liveness/readiness）--终止前（prestop）--终止
启动后钩子干的事情很少用到，主要是停止前。

```shell
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod1
  name: pod1
spec:
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: c1
    resources: {}
    lifecycle:
      postStart:
        exec:
          command:
          - /bin/sh
          - -c
          - "echo hahahaha > /aaa.txt"
      preStop:
        exec:
          command:
          - /bin/sh
          - -c
          - "sleep 10"
```

# 静态pod
思考一问题：搭建好的一套k8s集群，之后在这个集群中创建一系列pod用于支撑业务，但是这套k8s集群是怎么起来的？通过一系列pod起来的。

在/etc/kubernetes/manifest下面默认会有4个yaml文件，这里面定义的所有的pod，会被自动创建，移走自动删除。你也可以尝试手工单独创建一个yaml文件看效果。
静态pod用于支撑k8s集群核心功能而存在。

# pod 标签
标签格式：
以k-v键值对的形式存在，以等号左右分开
如：xxx=yyy  xxx.yyy=zzz  aaa.bbb/ccc=ddd

## 查看节点标签
获取节点的标签
```shell
[root@master ~]# kubectl get nodes --show-labels
NAME     STATUS   ROLES           AGE   VERSION   LABELS
master   Ready    control-plane   82d   v1.31.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
node1    Ready    <none>          82d   v1.31.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux
node2    Ready    <none>          82d   v1.31.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node2,kubernetes.io/os=linux``
```
## 为节点打标签
```shell
[root@master ~]# kubectl label nodes node2 disk=ssd
node/node2 labeled
[root@master ~]# kubectl get nodes node2 --show-labels
NAME    STATUS   ROLES    AGE   VERSION   LABELS
node2   Ready    <none>   82d   v1.31.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disk=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=node2,kubernetes.io/os=linux
```

## 为节点删除标签
```shell
[root@master ~]# kubectl label nodes node2 disk-
node/node2 unlabeled
[root@master ~]# kubectl get nodes node2 --show-labels
NAME    STATUS   ROLES    AGE   VERSION   LABELS
node2   Ready    <none>   82d   v1.31.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node2,kubernetes.io/os=linux
```

## 查看pod标签
```shell
[root@master ~]# kubectl get pod --show-labels
NAME   READY   STATUS    RESTARTS   AGE   LABELS
pod1   1/1     Running   0          6s    run=pod1
```

## 为pod打标签
```shell
[root@master ~]# kubectl label pods pod2 aaa=bbb
pod/pod2 labeled
[root@master ~]# kubectl get pod --show-labels
NAME   READY   STATUS    RESTARTS   AGE    LABELS
pod1   1/1     Running   0          2m6s   run=pod1
pod2   1/1     Running   0          27s    aaa=bbb,run=pod2
```

## 为pod删除标签
```shell
[root@master ~]# kubectl label pods pod2 aaa-
pod/pod2 unlabeled
[root@master ~]# kubectl get pod --show-labels
NAME   READY   STATUS    RESTARTS   AGE     LABELS
pod1   1/1     Running   0          2m50s   run=pod1
pod2   1/1     Running   0          71s     run=pod2
```

思考：为node2节点添加一个disk=ssd标签，为pod2也添加一个disk=ssd标签，问？这个pod是不是会被调度到node2上？
指定pod到对应节点上，可以通过定义节点标签，并在pod中添加nodeSelector标签选择器来实现。
```shell
[root@master ~]# cat pod2.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod2
    disk: ssd
  name: pod2
spec:
  nodeSelector:
    disk: ssd
  containers:
  - image: nginx
    name: pod2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

## node特殊标签
定义身份角色的标签，没有特殊意义，用途仅仅就是为了好区分。
```shell
[root@master ~]# kubectl label nodes master node-role.kubernetes.io/master=
node/master labeled
[root@master ~]# kubectl get node
NAME     STATUS   ROLES                  AGE   VERSION
master   Ready    control-plane,master   82d   v1.31.3
node1    Ready    <none>                 82d   v1.31.3
node2    Ready    <none>                 82d   v1.31.3
[root@master ~]# kubectl label nodes master node-role.kubernetes.io/control-plane-
node/master unlabeled
[root@master ~]# kubectl get node
NAME     STATUS   ROLES    AGE   VERSION
master   Ready    master   82d   v1.31.3
node1    Ready    <none>   82d   v1.31.3
node2    Ready    <none>   82d   v1.31.3

[root@master ~]# kubectl label nodes node1 node-role.kubernetes.io/n1=
node/node1 labeled
[root@master ~]# kubectl label nodes node2 node-role.kubernetes.io/n2=
node/node2 labeled
[root@master ~]# kubectl get nodes
NAME     STATUS   ROLES    AGE   VERSION
master   Ready    master   82d   v1.31.3
node1    Ready    n1       82d   v1.31.3
node2    Ready    n2       82d   v1.31.3
```
# pod 调度
## pod cordon（警戒线）
cordon相当于给节点拉起一条警戒线。
主要是为了临时维护或更新某些操作，某段时间内不允许再往该节点上调度pod。对于当前节点上已经存在且运行的pod不受影响。

```shell
[root@master ~]# kubectl cordon node2
node/node2 cordoned
[root@master ~]# kubectl get nodes
NAME     STATUS                     ROLES    AGE   VERSION
master   Ready                      master   82d   v1.31.3
node1    Ready                      n1       82d   v1.31.3
node2    Ready,SchedulingDisabled   n2       82d   v1.31.3

[root@master ~]# kubectl uncordon node2
node/node2 uncordoned
```

## pod drain（警戒线+驱逐）
drain这个操作包含了两个动作（cordon-->evicted）
这里的驱逐evicted动作，只能在控制器中，例如deployment中看到效果，如果pod是手工创建的，看不到效果（效果等同于删除）。对于当前节点上已存在且运行的pod会进行删除/驱逐。

为了看清drain操作效果，使用deployment控制器来操作。
- 流程：首先在当前节点拉起警戒线cordon，然后把当前节点上除了管理型daemonset的pod不被删除以外，把所有业务行pod全部删除，最后把这些pod在其他节点上重建出来。

1.创建一个deploy
```shell
[root@master ~]# kubectl create deployment deploy1 --image nginx --replicas 3 --dry-run=client -o yaml > deploy1.yaml
[root@master ~]# cat deploy1.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: deploy1
  name: deploy1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: deploy1
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: deploy1
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}

[root@master ~]# kubectl apply -f deploy1.yaml
[root@master ~]# kubectl get pod -o wide
NAME                      READY   STATUS    RESTARTS   AGE     IP               NODE    NOMINATED NODE   READINESS GATES
deploy1-d7b74cb77-cbb4c   1/1     Running   0          17s     10.244.104.31    node2   <none>           <none>
deploy1-d7b74cb77-ffx7g   1/1     Running   0          17s     10.244.166.147   node1   <none>           <none>
deploy1-d7b74cb77-kzdn4   1/1     Running   0          17s     10.244.166.149   node1   <none>           <none>
```

2.将node1节点进行drain操作
```shell
[root@master ~]# kubectl drain node1 --ignore-daemonsets --delete-emptydir-data --force

观察pod所在节点
[root@master ~]# kubectl get pod -o wide
NAME                      READY   STATUS    RESTARTS   AGE     IP              NODE    NOMINATED NODE   READINESS GATES
deploy1-d7b74cb77-6w98h   1/1     Running   0          21s     10.244.104.34   node2   <none>           <none>
deploy1-d7b74cb77-cbb4c   1/1     Running   0          7m33s   10.244.104.31   node2   <none>           <none>
deploy1-d7b74cb77-nz28z   1/1     Running   0          21s     10.244.104.33   node2   <none>           <none>

[root@master ~]# kubectl uncordon node1
node/node1 uncordoned
[root@master ~]# kubectl get nodes
NAME     STATUS   ROLES    AGE   VERSION
master   Ready    master   82d   v1.31.3
node1    Ready    n1       82d   v1.31.3
node2    Ready    n2       82d   v1.31.3
```
必须忽略 daemonset
## pod taint (污点)
taint 污点，前科
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250224150134517.png)

查看污点
```shell
[root@master ~]# kubectl describe nodes master |grep -i taint
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
[root@master ~]# kubectl describe nodes node1 |grep -i taint
Taints:             <none>
[root@master ~]# kubectl describe nodes node2 |grep -i taint
Taints:             <none>

为node2添加一个污点
[root@master ~]# kubectl taint node node2 aaa=bbb:NoSchedule
node/node2 tainted
[root@master ~]# kubectl describe nodes node2 |grep -i taint
Taints:             aaa=bbb:NoSchedule

[root@master ~]# kubectl taint node node2 aaa-
node/node2 untainted
[root@master ~]# kubectl describe nodes node2 |grep -i taint
Taints:             <none>
```
还是可以在污点节点添加 pod，需要配合容忍度使用

这个公司确实有很多污点：pua，997，不按时发工资....
明天继续。


