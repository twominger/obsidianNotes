# 关于pod的调度
cordon/drain/taint

[root@master ~]# kubectl taint node node1 aaa=bbb:NoSchedule
node/node1 tainted
[root@master ~]# kubectl describe nodes node1 |grep -i taint
Taints:             aaa=bbb:NoSchedule  老板画饼

[root@master ~]# kubectl taint node node2 aaa=ccc:NoSchedule
node/node2 tainted
[root@master ~]# kubectl describe nodes node2 |grep -i taint
Taints:             aaa=ccc:NoSchedule   老板pua

默认master本身也有污点，3台都有污点，直接创建pod，会pending
污点会配合着容忍度 toleration 一起使用

tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"

tolerations:
- key: "key1"
  operator: "Exists"
  effect: "NoSchedule"

[root@master ~]# cat pod1.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod1
  name: pod1
spec:
  containers:
  - image: nginx
    name: pod1
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  tolerations:
  - key: "aaa"
    operator: "Equal"
    value: "ccc"
    effect: "NoSchedule"
status: {}

[root@master ~]# cat pod2.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod2
  name: pod2
spec:
  containers:
  - image: nginx
    name: pod2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  tolerations:
  - key: "aaa"
    operator: "Exists"
    effect: "NoSchedule"
status: {}

也可以根据实际情况写多个容忍度，在NoSchedule情况下，只需满足任意一个即可调度
[root@master ~]# cat pod1.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod1
  name: pod1
spec:
  containers:
  - image: nginx
    name: pod1
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  tolerations:
  - key: "aaa"
    operator: "Equal"
    value: "ccc"
    effect: "NoSchedule"
  - key: "kkk"
    operator: "Equal"
    value: "ggg"
    effect: "NoSchedule"
status: {}

# k8s 存储管理
本地存储：
emptyDir/hostPath
网络存储：
NFS，SAN等
持久性存储（重点）：
PV/PVC

网络存储NFS：
1.准备一台linux（50G）
2.分区格式化 10G
3.文件系统挂载
4.配置yum源安装软件包
yum install -y yum-utils vim bash-completion net-tools wget nfs-utils

5.启动nfs服务
systemctl enable nfs-server.service
systemctl start nfs-server.service

6.关闭防火墙
systemctl disable firewalld.service
systemctl stop firewalld.service
setenforce 0
vim /etc/selinux/config

SELINUX=disabled

7.配置exports
[root@nfs ~]# vim /etc/exports
[root@nfs ~]# cat /etc/exports
/data *(rw,async,no_root_squash)

exportfs -arv //不用重启nfs服务，配置文件就会生效
no_root_squash：登入 NFS 主机使用分享目录的使用者，如果是 root 的话，那么对于这个分享的目录来说，他就具有 root 的权限！这个项目『极不安全』，不建议使用。以root身份写。
exportfs命令
-a 全部挂载或者全部卸载
-r 重新挂载
-u 卸载某一个目录
-v 显示共享目录

8.测试访问
k8s集群节点（node节点）安装nfs-utils
[root@node2 ~]# mkdir /test
[root@node2 ~]# mount 192.168.44.245:/data /test
[root@node2 ~]# df -Th

9.编辑yaml创建pod测试
注意：把之前的调度策略取消

[root@master ~]# kubectl run pod1 --image nginx --image-pull-policy IfNotPresent --dry-run=client -o yaml > pod1.yaml
[root@master ~]# vim pod1.yaml
[root@master ~]# cat pod1.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod1
  name: pod1
spec:
  volumes:
  - name: v1
    emptyDir: {}
  - name: v2
    hostPath:
      path: /data
  - name: v3
    nfs:
      server: 192.168.44.245
      path: /data
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: pod1
    resources: {}
    volumeMounts:
    - name: v3
      mountPath: /usr/share/nginx/html/
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

[root@master ~]# kubectl exec -ti pod1 -- bash
root@pod1:/# cd /usr/share/nginx/html/
root@pod1:/usr/share/nginx/html# ls
root@pod1:/usr/share/nginx/html# touch {a,b,c}{1,2,3}.txt
root@pod1:/usr/share/nginx/html# ls
a1.txt  a2.txt  a3.txt  b1.txt  b2.txt  b3.txt  c1.txt  c2.txt  c3.txt

[root@nfs data]# ls
a1.txt  a2.txt  a3.txt  b1.txt  b2.txt  b3.txt  c1.txt  c2.txt  c3.txt

## k8s 持久化存储
PV、PVC不是一种新的存储类型，而是k8s用来做存储管理的一种资源对象。（先规划、再申请、最后使用）
PV persistent volume  持久卷
PVC persistent volume claim 持久卷申领

1.准备一个目录空间
用刚才的NFS，再创建一个5G分区。
[root@nfs ~]# cat /etc/exports
/data *(rw,async,no_root_squash)
/vol1 *(rw,async,no_root_squash)

[root@nfs ~]# exportfs -arv
exporting *:/vol1
exporting *:/data

2.创建PV
编辑yaml模板，可参考官方文档
https://kubernetes.io/docs/concepts/storage/persistent-volumes/
https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/

[root@master ~]# vim pv01.yaml
[root@master ~]# cat pv01.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv01
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: manual
  nfs:
    path: "/vol1"
    server: 192.168.44.245

这里面有个accessModes访问模式
访问模式有：

ReadWriteOnce
卷可以被一个节点以读写方式挂载。 ReadWriteOnce 访问模式仍然可以在同一节点上运行的多个 Pod 访问（读取或写入）该卷。 对于单个 Pod 的访问，请参考 ReadWriteOncePod 访问模式。
ReadOnlyMany
卷可以被多个节点以只读方式挂载。
ReadWriteMany
卷可以被多个节点以读写方式挂载。
ReadWriteOncePod
特性状态： Kubernetes v1.29 [stable]
卷可以被单个 Pod 以读写方式挂载。 如果你想确保整个集群中只有一个 Pod 可以读取或写入该 PVC， 请使用 ReadWriteOncePod 访问模式。

在命令行接口（CLI）中，访问模式也使用以下缩写形式：

RWO - ReadWriteOnce
ROX - ReadOnlyMany
RWX - ReadWriteMany
RWOP - ReadWriteOncePod

[root@master ~]# kubectl apply -f pv01.yaml
persistentvolume/pv01 created
[root@master ~]# kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
pv01   5Gi        RWO            Retain           Available           manual         <unset>                          2s

RECLAIM POLICY 回收策略：常用两种 retain/delete
Retain（静态模式）：保留，当删除pod，删除pvc，删除pv的时候，底层数据依然保留。优点：可以防止误删除；缺点：需要定期清理无用的数据，占用大量存储空间。
Delete（动态模式）：删除，当删除pod，删除pvc，自动删除pv，自动删除底层数据。优点：及时清理了无效数据；缺点：如果误删除，数据丢失。

注意：一个pv是否可以被pvc进行申领，就是看status是否为 Available

3.创建pvc
[root@master ~]# vim pvc01.yaml
[root@master ~]# cat pvc01.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc01
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

[root@master ~]# kubectl apply -f pvc01.yaml
persistentvolumeclaim/mypvc01 created
[root@master ~]# kubectl get pvc
NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
mypvc01   Bound    pv01     5Gi        RWO            manual         <unset>                 3s

[root@master ~]# kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
pv01   5Gi        RWO            Retain           Bound    default/mypvc01   manual         <unset>                          11m

假如创建了很多个pv，你创建一个pvc的时候，到底会自动关联哪个pv呢？
storageClassName: manual
accessModes:ReadWriteOnce
requests:storage: 5Gi

动态卷制备
通过三方驱动可以实现，创建pvc的时候自动创建pv，删除pvc的时候，自动删除pv及底层数据（对应回收策略：delete）
底层如何关联，以及pv如何对接，这些全部都在yaml文件中定义好了。


