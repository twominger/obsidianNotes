一、存储池操作
1. 创建一个名为 pool01 的复制池应用类型为 rbd，该池需要被保护不允许用户删除
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311164720080.png)
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311164847739.png)
2. 使用 rados 将系统中的/etc/passwd 到 pool01 的存储池中，并将对象名称设置为 object1
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311165518858.png)
3. 将 object1 下载到本机的/opt/test 目录中命名为 download_object.txt
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311165820268.png)
4. 查询 object1 在 ceph 集群中所在的 pg，并找到 pg 所在的 OSD，并根据主 osd 找到所在节点的磁盘
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311170642895.png)
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311171516727.png)
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311171538562.png)
5. 为 pool01 的存储池开启对象配额和容量配额，对象配额为 10 个对象、容量配额为 100M；并进行测试
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311172150694.png)
6. 创建一个名为 pool02 的存储池，类型为复制池，调整该池最多允许两个副本故障，并调整该池的 pg 数量为 64
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311173505307.png)
7. 为 ceph 集群配置一条纠删码规则，规则的名称为 eccode，该纠删码规则将一个对象切分为 3 个数据块，并生成两个校验快；使用 OSD 作为故障冗余域
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311173854319.png)
8. 使用配置的 eccode 为集群创建一个名为 ecpool 的存储池，该池的类型为纠删码池
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311174225738.png)
9. 在 ecpool 中上传一个名为 demo. db 的对象，该对象来自于系统中的/var/lib/rpm/Index. db，并查看该对象实际占用的集群存储空间大小
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311174637171.png)
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311174934888.png)
10. 为 pool01 拍摄一个名为 snap01 的快照，并删除存储池中的 object1，删除后使用 snap01 的快照还原该对象
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311175545023.png)
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311180437182.png)
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311180656861.png)
11. 在 pool01 中存在一个名为 sp01 的命名空间，该命名空间中存在来自于/etc/profile 的对象 profile，配置并查看该对象，同时使用 json 的格式输出该对象
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311181703236.png)
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250311182035714.png)
二、ceph 的集群配置
12. 在 node1 上为 zhangsan 用户配置集群管理员权限，zhangsan，可以使用家目录中的 ceph 的配置文件访问到一个名为 storage1 的 ceph 集群
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250313153057496.png)
13. 使用 tell 临时修改三个 mon 的配置，使得存储池 ecpool 能够被删除
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250313154334630.png)
14. 使用 orch 重启集群所有 mon 节点验证 mon_allow_pool_delete 是否配置还原
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250313154405984.png)
15. 通过 cephadm 压缩 mon 节点上的集中式数据库
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250313154640003.png)
16. 配置集群参数监控集中式数据库，当集中式数据库达到 20G 时集群进入告警状态
![image.png](https://notes-ming.oss-cn-beijing.aliyuncs.com/images/20250313155232862.png)


